{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions used during data preprocessing, training, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use the tokenizer to encode the text\n",
    "def encode(data_component):\n",
    "        encoded_data = tokenizer(data_component, return_tensors='pt', padding=True)\n",
    "        return encoded_data\n",
    "\n",
    "\n",
    "def qa_pairs(questions, options):\n",
    "    pairs = []\n",
    "    for q, opts in zip(questions, options):\n",
    "        for opt in opts:\n",
    "            pairs.append((q,opt))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "##outputs the accuracy of the model\n",
    "def accuracy(predictions, labels):\n",
    "    preds = torch.argmax(predictions, dim=1)  # Get class with highest probability\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "\n",
    "##Training Loop\n",
    "def train(bert, device, training_data, criterion, optimizer, epoch):\n",
    "    #Set Model to training mode\n",
    "    bert_classifier.train()\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0  # To accumulate loss for this epoch\n",
    "        training_acc = 0.0  # Accumulate accuracy\n",
    "        training_loss = 0.0\n",
    "    \n",
    "    #loop over the batches of data:\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            inputs = {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "        \n",
    "        \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            #forward pass\n",
    "            outputs = bert(inputs) #get the output logits from the model\n",
    "            logits = bert_classifier(inputs)\n",
    "            \n",
    "            \n",
    "            #calculate the loss\n",
    "            loss = criterion(logits, labels) # Calculate the loss\n",
    "            \n",
    "            \n",
    "            #Backward pass and optimize\n",
    "            loss.backward() #compute the gradients\n",
    "            optimizer.step() # Update the parameters\n",
    "            \n",
    "            \n",
    "            # Accumulate loss for this batch\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            #Print the statistics\n",
    "            training_loss += loss.item()\n",
    "            training_acc+= accuracy(outputs, labels)# accumulate the accuracy and loss\n",
    "            if (i + 1) % 200 == 0: # print every two hundred batches\n",
    "                print(f'Epoch {epoch}, Batch {i+1}, Loss: {training_loss / 200:.4f}, Accuracy: {training_acc / 200:.4f}')\n",
    "\n",
    "                training_loss = 0.0\n",
    "                training_acc = 0.0\n",
    "\n",
    "\n",
    "#Testing Loop        \n",
    "def test(model, device, testloader, criterion, epoch):\n",
    "    # Set the model to evaluation mode\n",
    "    bert_classifier.eval()\n",
    "    TestAccuracy = []\n",
    "    correct_predictions = ()\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0  # To accumulate loss for this epoch\n",
    "        testing_acc = 0.0  # Accumulate accuracy\n",
    "        testing_loss = 0.0\n",
    "        \n",
    "        \n",
    "\n",
    "    #loop over the batches of data:\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(testloader):\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device)\n",
    "                inputs = {\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask\n",
    "                }\n",
    "                \n",
    "                \n",
    "                #forward pass\n",
    "                outputs = model(inputs)\n",
    "                logits = model(inputs)\n",
    "            \n",
    "                \n",
    "                #calculate the loss\n",
    "                loss = criterion(logits, labels) # Calculate the loss\n",
    "               \n",
    "                \n",
    "                # Accumulate loss for this batch\n",
    "                epoch_loss += loss.item()\n",
    "                testing_loss += loss.item()\n",
    "                testing_acc+= accuracy(outputs, labels)\n",
    "\n",
    "                                \n",
    "                if (i + 1) % 200 == 0: # print every two hundred batches\n",
    "                    print(f'Epoch {epoch}, Batch {i+1}, Loss: {testing_loss / 200:.4f}, Accuracy: {testing_acc / 200:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import random_split \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput\n",
    "from transformers.models.bert.modeling_bert import BertEncoder\n",
    "from transformers.models.bert.modeling_bert import BertModel, BertConfig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the question sets that will be used to train the model.  The first dataset used was the RACE dataseet, which consists of multiple choice questions separated between M (middle school) and H (high school).  The second dataset is QxGrade_dataset which is a set of 75k questions scraped from pdf textbooks.  These textbooks were chosen based on alignment with Common Core State Standards to identify a framework that we can use when training the model with additional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('QxGrade_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important columns we will be using and labeling are Grade and Question.  Using the .values and .tolist function here we are adding all of the grade options (3-12) to the grades function.  We are doing the same with all of the question values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions = df.Question.values.tolist()\n",
    "#grades = df.Grade.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use QA pairs to create a list of questions and their grade levels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa_pairs = qa_pairs(grades, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data using the encode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1017,  102, 1015,  102],\n",
       "        [ 101, 1017,  102, 1012,  102],\n",
       "        [ 101, 1017,  102, 1014,  102],\n",
       "        ...,\n",
       "        [ 101, 1022,  102, 1050,  102],\n",
       "        [ 101, 1022,  102, 1037,  102],\n",
       "        [ 101, 1022,  102, 1050,  102]]), 'token_type_ids': tensor([[0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qa_pairs = encode(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the qa_pairs using torch.save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(qa_pairs, \"qa_pairs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the grade values to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = 'qa_pairs.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glabels = torch.zeros(len(categories), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the question values to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = torch.load(\"questions.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabels = torch.ones(len(questions), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.cat([qlabels, glabels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the proportion of the data you want to use for training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = training_data.shape[0]\n",
    "train_size = int(.7 * total_size)\n",
    "val_size = int(.2 * total_size)\n",
    "test_size = total_size - train_size - val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Random_Split to chunk the training data into training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = random_split(training_data, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5  ##How many times we go through the loop\n",
    "criterion = nn.CrossEntropyLoss()  ##This compares the predicted answer with the correct answer\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-5)  ##The Optim.Adam optimizer calculates gradient descent\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  ##Set the device to GPU so we can train the model on the GPU\n",
    "\n",
    "sequence_length = 200   ## Maximum length of tokens to be used at a time\n",
    "batch_size = 64  ##The number of training examples in one forward/backward pass\n",
    "input_dim = 500  ##The total number of dimension we will allow the model to use for calculation\n",
    "d_model = 512  ##Number of expected features, set to default recommended by pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change BERT configuration to implement Dual Multihead Attention Mechanism.  This is also where we are implementing our customized neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This implements a copy of the original attention mechnism to run simultaneously, then at the end, the outputs are joined together\n",
    "class DualBertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention1 = BertSelfAttention(config)\n",
    "        self.attention2 = BertSelfAttention(config)\n",
    "        \n",
    "        self.output1 = BertSelfOutput(config)\n",
    "        self.output2 = BertSelfOutput(config)\n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        attn_output1, _ = self.attention1(hidden_states, attention_mask, head_mask)\n",
    "        attn_output1 = self.output1(attn_output1, hidden_states)\n",
    "        \n",
    "        attn_output2, _ = self.attention2(hidden_states, attention_mask, head_mask)\n",
    "        attn_output2 = self.output2(attn_output2, hidden_states)\n",
    "        dual_attention_output = F.relu(attn_output1 + attn_output2)\n",
    "        return dual_attention_output\n",
    "    \n",
    "##Implements the dual attention in the Neural Network\n",
    "class DualBertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = DualBertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "\n",
    "##Moves the data through the Neural Network\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask, head_mask)  ##Layer 1\n",
    "        intermediate_output = self.intermediate(attention_output)  ##Layer 2\n",
    "        layer_output = self.output(intermediate_output, attention_output)  \n",
    "        return layer_output\n",
    "\n",
    "\n",
    "##Implments the outcome from the DualBertLayer to encode the data from the DualBertLayer Class\n",
    "class DualBertEncoder(BertEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([DualBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "##Implements the model with the DualBertEncoder\n",
    "class DualBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = DualBertEncoder(config)\n",
    "\n",
    "\n",
    "##Initialize a classifier between the 10 different options (3rd grade to 12th grade).\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim=768, num_classes=10): \n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the data in a Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(training_data, (training_data != 0).long(), labels) \n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "testloader = DataLoader(dataset, batch_size=32, shuffle=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the classifier to the GPU as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = BertClassifier(bert).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bert to classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(bert_classifier, device, trainloader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
