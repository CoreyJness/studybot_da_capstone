{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions used during data preprocessing, training, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##Use the tokenizer to encode the text\n",
    "def encode(data_component):\n",
    "    encoded_data = tokenizer(data_component, return_tensors='pt', padding=\"max_length\", truncation=True, max_length=512)  #Force max length here to make sure tensors fit\n",
    "    return encoded_data[\"input_ids\"], encoded_data[\"attention_mask\"]\n",
    "\n",
    "\n",
    "##outputs the accuracy of the model\n",
    "def accuracy(predictions, labels):\n",
    "    preds = torch.argmax(torch.softmax(predictions, dim=1), dim=1)  # Get class with highest probability\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "\n",
    "##Training Loop\n",
    "def train(bert_classifier, device, trainloader, valloader, criterion, optimizer, epochs):\n",
    "    running_acc = []  # Store accuracy values for logging\n",
    "    running_train_loss = []  # Store training loss for visualization\n",
    "    running_val_loss = []  # Store validation loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0  \n",
    "        training_acc = 0.0  \n",
    "        training_loss = 0.0\n",
    "        \n",
    "        bert_classifier.train()  # Set model to training mode\n",
    "\n",
    "        # Training loop\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device).long()  \n",
    "            \n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = bert_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights\n",
    "            \n",
    "            # Accumulate loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            training_loss += loss.item()\n",
    "            training_acc += accuracy(logits, labels)  # Assume `accuracy` function is defined\n",
    "\n",
    "            # Print loss and accuracy every 50 batches\n",
    "            if (i + 1) % 50 == 0: \n",
    "                avg_acc = training_acc / 50  # Normalize by batch count\n",
    "                avg_loss = training_loss / 50\n",
    "\n",
    "                print(f'Epoch {epoch}, Batch {i+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}')\n",
    "                \n",
    "                running_acc.append(avg_acc)\n",
    "\n",
    "                # Reset batch accumulators\n",
    "                training_loss = 0.0\n",
    "                training_acc = 0.0\n",
    "\n",
    "        # Store training loss for visualization\n",
    "        avg_train_loss = epoch_loss / len(trainloader)\n",
    "        running_train_loss.append(avg_train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        running_vloss = 0.0\n",
    "        bert_classifier.eval()  # Set model to evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_acc = 0.0\n",
    "            val_steps = 0\n",
    "            \n",
    "            for vbatch in valloader:\n",
    "                vinputs = vbatch[0].to(device)\n",
    "                vattention_mask = vbatch[1].to(device)\n",
    "                vlabels = vbatch[2].to(device).long()\n",
    "\n",
    "                vlogits = bert_classifier(input_ids=vinputs, attention_mask=vattention_mask)\n",
    "                vloss = criterion(vlogits, vlabels)\n",
    "                \n",
    "                running_vloss += vloss.item()\n",
    "                val_acc += accuracy(vlogits, vlabels)  \n",
    "                val_steps += 1\n",
    "\n",
    "            avg_vloss = running_vloss / val_steps\n",
    "            avg_vacc = val_acc / val_steps\n",
    "            running_val_loss.append(avg_vloss)\n",
    "\n",
    "            print(f'Epoch {epoch}, Training Loss: {avg_train_loss:.4f}, '\n",
    "                  f'Validation Loss: {avg_vloss:.4f}, Validation Accuracy: {avg_vacc:.4f}')\n",
    "\n",
    "    # **Create DataFrames for Plotting**\n",
    "    df_loss = pd.DataFrame({\n",
    "        'Epoch': range(1, epochs + 1),\n",
    "        'Training Loss': running_train_loss,\n",
    "        'Validation Loss': running_val_loss\n",
    "    })\n",
    "\n",
    "    df_acc = pd.DataFrame({'Batch': range(len(running_acc)), 'Accuracy': running_acc})\n",
    "\n",
    "\n",
    "    df_loss.plot(x='Epoch', y=['Training Loss', 'Validation Loss'], kind='line', marker='o', title='Training vs Validation Loss', grid=True)\n",
    "    df_acc.plot(x='Batch', y='Accuracy', kind='line', title='Training Accuracy Over Batches', grid=True)\n",
    "\n",
    "\n",
    "#Testing Loop        \n",
    "def test(bert_classifier, device, testloader, criterion, epochs):\n",
    "    bert_classifier.eval()  # Set model to evaluation mode\n",
    "\n",
    "    running_test_loss = []  # Store test loss per epoch\n",
    "    running_test_acc = []  # Store test accuracy per batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0.0  \n",
    "            testing_acc = 0.0  \n",
    "            testing_loss = 0.0  \n",
    "            batch_count = 0  \n",
    "\n",
    "            for i, batch in enumerate(testloader):\n",
    "                input_ids = batch[0].to(device)\n",
    "                attention_mask = batch[1].to(device)\n",
    "                labels = batch[2].to(device).long()\n",
    "\n",
    "                # Forward pass\n",
    "                logits = bert_classifier(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Accumulate loss and accuracy\n",
    "                epoch_loss += loss.item()\n",
    "                testing_loss += loss.item()\n",
    "                testing_acc += accuracy(logits, labels)\n",
    "                batch_count += 1\n",
    "\n",
    "                # Print loss and accuracy every 200 batches\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    avg_acc = testing_acc / 200\n",
    "                    avg_loss = testing_loss / 200\n",
    "                    print(f'Epoch {epoch}, Batch {i+1}, Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f}')\n",
    "                    \n",
    "                    running_test_acc.append(avg_acc)\n",
    "\n",
    "                    # Reset batch accumulators\n",
    "                    testing_loss = 0.0\n",
    "                    testing_acc = 0.0\n",
    "\n",
    "            # Store test loss per epoch\n",
    "            avg_test_loss = epoch_loss / batch_count\n",
    "            running_test_loss.append(avg_test_loss)\n",
    "\n",
    "            print(f'Epoch {epoch}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # **Create DataFrames for Plotting**\n",
    "    df_loss = pd.DataFrame({'Epoch': range(1, epochs + 1), 'Test Loss': running_test_loss})\n",
    "    df_acc = pd.DataFrame({'Batch': range(len(running_test_acc)), 'Test Accuracy': running_test_acc})\n",
    "\n",
    "    # **Plot the test results**\n",
    "    df_loss.plot(x='Epoch', y='Test Loss', kind='line', marker='o', title='Test Loss Over Epochs', grid=True)\n",
    "    df_acc.plot(x='Batch', y='Test Accuracy', kind='line', title='Test Accuracy Over Batches', grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split \n",
    "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput, BertEncoder, BertSelfAttention, BertSelfOutput, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the question sets that will be used to train the model.  The first dataset used was the RACE dataseet, which consists of multiple choice questions separated between M (middle school) and H (high school).  The second dataset is QxGrade_dataset which is a set of 75k questions scraped from pdf textbooks.  These textbooks were chosen based on alignment with Common Core State Standards to identify a framework that we can use when training the model with additional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'QxGrade_Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQxGrade_Dataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\corey\\Documents\\Projects\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\corey\\Documents\\Projects\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\corey\\Documents\\Projects\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\corey\\Documents\\Projects\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\corey\\Documents\\Projects\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'QxGrade_Dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('QxGrade_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important columns we will be using and labeling are Grade and Question.  Using the .values and .tolist function here we are adding all of the grade options (3-12) to the grades function.  We are doing the same with all of the question values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df.question.values.tolist()\n",
    "grades = df.Grade.astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data using the encode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tensors, attention_mask_q = encode(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tensors, attention_mask_g = encode(grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the grade values to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glabels = torch.zeros(len(g_tensors), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the question values to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlabels = torch.ones(len(q_tensors), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = torch.cat([g_tensors, q_tensors], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change BERT configuration to implement Dual Multihead Attention Mechanism.  This is also where we are implementing our customized neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This implements a copy of the original attention mechnism to run simultaneously, then at the end, the outputs are joined together\n",
    "class DualBertAttention(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention1 = BertSelfAttention(config)\n",
    "        self.attention2 = BertSelfAttention(config)\n",
    "        \n",
    "        self.output1 = BertSelfOutput(config)\n",
    "        self.output2 = BertSelfOutput(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        attn_output1 = self.attention1(hidden_states, attention_mask, head_mask)[0]  # Unpacking tuple\n",
    "        attn_output1 = self.output1(attn_output1, hidden_states)\n",
    "\n",
    "        attn_output2 = self.attention2(hidden_states, attention_mask, head_mask)[0]  # Unpacking tuple\n",
    "        attn_output2 = self.output2(attn_output2, hidden_states)\n",
    "\n",
    "        dual_attention_output = F.relu(attn_output1 + attn_output2)\n",
    "        return dual_attention_output\n",
    "    \n",
    "##Implements the dual attention in the Neural Network\n",
    "class DualBertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = DualBertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "\n",
    "##Moves the data through the Neural Network\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        attention_output = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return (layer_output,)\n",
    "\n",
    "\n",
    "##Implments the outcome from the DualBertLayer to encode the data from the DualBertLayer Class\n",
    "class DualBertEncoder(BertEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([DualBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "##Implements the model with the DualBertEncoder\n",
    "class DualBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = DualBertEncoder(config)\n",
    "\n",
    "\n",
    "##Initialize a classifier between the 10 different options (3rd grade to 12th grade).\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim=768, num_classes=10): \n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # CLS token representation\n",
    "        return self.fc(pooled_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the network we create, instantiate our version of bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig()  # This needs to be an instantiated object, not a class reference\n",
    "bert = DualBertModel(config)  # Directly create an instance instead of calling from_pretrained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5  ##How many times we go through the loop\n",
    "criterion = nn.CrossEntropyLoss()  ##This compares the predicted answer with the correct answer\n",
    "optimizer = optim.Adam(bert.parameters(), lr=1e-5, betas=(0.9, 0.999), weight_decay=1e-5)  ##The Optim.Adam optimizer calculates gradient descent\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  ##Set the device to GPU so we can train the model on the GPU\n",
    "\n",
    "sequence_length = 156   ## Maximum length of tokens to be used at a time\n",
    "batch_size = 16  ##The number of training examples in one forward/backward pass\n",
    "input_dim = 500  ##The total number of dimension we will allow the model to use for calculation\n",
    "d_model = 512  ##Number of expected features, set to default recommended by pytorch\n",
    "running_acc = []  ##This is to agregate the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the data in a Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "q_dataset = TensorDataset(q_tensors, attention_mask_q, qlabels.long())  # Convert qlabels to long\n",
    "g_dataset = TensorDataset(g_tensors, attention_mask_g, glabels.long())  # Convert glabels to l\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([q_dataset, g_dataset])\n",
    "\n",
    "\n",
    "#split datasets with random split\n",
    "train_size = int(0.7 * len(dataset))  # 70% training\n",
    "val_size = int(0.15 * len(dataset))   # 15% validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining 15% test\n",
    "\n",
    "train_data, val_data, test_data = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "##Reference torch.utils.data \n",
    "def custom_collate(batch):\n",
    "    inputs = torch.stack([b[0] for b in batch])  # Extract input tensors\n",
    "    attention_masks = torch.stack([b[1] for b in batch])  # Extract attention masks\n",
    "    labels = torch.stack([b[2] for b in batch])\n",
    "    # Default case: use PyTorch's default_collate\n",
    "    return inputs, attention_masks, labels \n",
    "\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "valloader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=custom_collate)  \n",
    "\n",
    "testloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move an instance of the bert classifier to the device as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = BertClassifier(bert).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bert to classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(bert_classifier, device, trainloader, valloader, criterion, optimizer, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model once trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_classifier.state_dict(), 'Bert_Classifier.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
