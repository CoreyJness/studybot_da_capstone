{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions used during data preprocessing, training, and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, input_ids, attention_mask, train_loader, criterion, optimizer, epochs):\n",
    "    running_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accuracy\n",
    "            _, predicted_labels = torch.max(outputs, dim=1)\n",
    "            correct_predictions = (predicted_labels == labels).sum().item()\n",
    "            total_correct += correct_predictions\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = total_correct / total_samples\n",
    "        running_acc.append(epoch_acc)\n",
    "\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    # Plotting after training\n",
    "    df_acc = pd.DataFrame({'Epochs': range(1, epochs + 1), 'Accuracy': running_acc})\n",
    "    df_acc.plot(x='Epochs', y='Accuracy', kind='line', title='Training Accuracy Over Epochs', grid=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device, epochs):\n",
    "    running_test_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for input_ids, attention_mask, labels in test_loader:\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accuracy\n",
    "                _, predicted_labels = torch.max(outputs, dim=1)\n",
    "                correct_predictions = (predicted_labels == labels).sum().item()\n",
    "                total_correct += correct_predictions\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_loss = total_loss / len(test_loader)\n",
    "        epoch_acc = total_correct / total_samples\n",
    "        running_test_acc.append(epoch_acc)\n",
    "\n",
    "        print(f'[Test] Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    # Plotting after testing\n",
    "    df_test = pd.DataFrame({'Epochs': range(1, epochs + 1), 'Accuracy': running_test_acc})\n",
    "    df_test.plot(x='Epochs', y='Accuracy', kind='line', title='Testing Accuracy Over Epochs', grid=True)\n",
    "    plt.show()\n",
    "    \n",
    "    return df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\corey\\Documents\\Projects\\studybot_da_capstone\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput, BertEncoder, BertSelfAttention, BertSelfOutput, BertModel, BertConfig, BertPooler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the question sets that will be used to train the model.  The first dataset used was the RACE dataseet, which consists of multiple choice questions separated between M (middle school) and H (high school).  The second dataset is QxGrade_dataset which is a set of 26k questions scraped from pdf textbooks.  These textbooks were chosen based on alignment with Common Core State Standards to identify a framework that we can use when training the model with additional data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('QxGrade_Dataset.csv')\n",
    "\n",
    "#target_size = 418\n",
    "#balanced_data = []\n",
    "\n",
    "#for grade, group in df.groupby(\"Grade\"):\n",
    "    #if len(group) >= target_size:\n",
    "        #sampled_group = group.sample(n=target_size, random_state=42)\n",
    "    #else:\n",
    "        #sampled_group = group\n",
    "    #balanced_data.append(sampled_group)\n",
    "\n",
    "#df_balanced = pd.concat(balanced_data).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I already ran through the whole process, I now know the model is training to specific levels too highly.  Now, I'm going to normalize the dataset to fix this.  To start we are going to see how many questions there are for each grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade Distribution:\n",
      " Grade\n",
      "3      418\n",
      "4     1191\n",
      "5     1658\n",
      "6      549\n",
      "7     1153\n",
      "8     1805\n",
      "9     8668\n",
      "10    2704\n",
      "11    7288\n",
      "12    1125\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "grade_counts = df['Grade'].value_counts().sort_index()\n",
    "print(\"Grade Distribution:\\n\", grade_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two most important columns we will be using and labeling are Grade and Question.  Using the .values and .tolist function here we are adding all of the grade options (3-12) to the grades function.  We are doing the same with all of the question values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.question.values.tolist()  ##X is questions\n",
    "y = df.Grade.astype(str).tolist() ##Y is answers\n",
    "\n",
    "num_classes = len(set(y))  ##This sets of the classification options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  ##Set the device to GPU so we can train the model on the GPU\n",
    "# Tokenize training and test sets\n",
    "x_train_encodings = tokenizer(x_train, padding=True, truncation=True, return_tensors='pt', max_length=32)\n",
    "x_test_encodings = tokenizer(x_test, padding=True, truncation=True, return_tensors='pt', max_length=32)\n",
    "\n",
    "# Extract input IDs and attention masks\n",
    "input_ids_train = x_train_encodings[\"input_ids\"].to(device)\n",
    "attention_mask_train = x_train_encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "input_ids_test = x_test_encodings[\"input_ids\"].to(device)\n",
    "attention_mask_test = x_test_encodings[\"attention_mask\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ints = [int(label) -3 for label in y_train] #Get grades below ten and change them to ints\n",
    "y_test_ints = [int(label) -3 for label in y_test]\n",
    "y_train_tensor = torch.tensor(y_train_ints)\n",
    "y_test_tensor = torch.tensor(y_test_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the modifications we made to the Bert model to implement Dual Multihead Attention Mechanisms and multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##This implements a copy of the original attention mechnism to run simultaneously, then at the end, the outputs are joined together\n",
    "class DualBertAttention(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention1 = BertSelfAttention(config)\n",
    "        self.attention2 = BertSelfAttention(config)\n",
    "        \n",
    "        self.output1 = BertSelfOutput(config)\n",
    "        self.output2 = BertSelfOutput(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        attn_output1 = self.attention1(hidden_states, attention_mask, head_mask)[0]  # Unpacking tuple\n",
    "        attn_output1 = self.output1(attn_output1, hidden_states)\n",
    "\n",
    "        attn_output2 = self.attention2(hidden_states, attention_mask, head_mask)[0]  # Unpacking tuple\n",
    "        attn_output2 = self.output2(attn_output2, hidden_states)\n",
    "\n",
    "        dual_attention_output = F.relu(attn_output1 + attn_output2)\n",
    "        return dual_attention_output\n",
    "    \n",
    "##Implements the dual attention in the Neural Network\n",
    "class DualBertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = DualBertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "\n",
    "##Moves the data through the Neural Network\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        attention_output = self.attention(hidden_states, attention_mask, head_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return (layer_output,)\n",
    "\n",
    "\n",
    "##Implments the outcome from the DualBertLayer to encode the data from the DualBertLayer Class\n",
    "class DualBertEncoder(BertEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.layer = nn.ModuleList([DualBertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "\n",
    "##Implements the model with the DualBertEncoder\n",
    "class DualBertModel(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = DualBertEncoder(config)\n",
    "        self.pooler = BertPooler(config)  # Add this line\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, **kwargs):\n",
    "        # Follow the same structure as the original BertModel\n",
    "        outputs = super().forward(input_ids=input_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  token_type_ids=token_type_ids,\n",
    "                                  **kwargs)\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        return (sequence_output, pooled_output)\n",
    "\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim=64, output_dim=6):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model  # Your DualBertModel\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs[0][:, 0, :]  # outputs[0] = last_hidden_state\n",
    "        return self.classifier(cls_embedding)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the network we created, instantiate our version of bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "dual_bert = DualBertModel(config)\n",
    "\n",
    "# Optionally initialize with pretrained BERT weights\n",
    "pretrained_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "dual_bert.embeddings = pretrained_bert.embeddings\n",
    "dual_bert.encoder.load_state_dict(pretrained_bert.encoder.state_dict(), strict=False)\n",
    "dual_bert.pooler = pretrained_bert.pooler\n",
    "\n",
    "# Now build the classifier\n",
    "bert_classifier = BertClassifier(dual_bert, 64, num_classes).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the hyperparameters.  You can tinker with training times, sizes, and loops here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 20 ##How many times we go through the loop\n",
    "test_epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()  ##This compares the predicted answer with the correct answer\n",
    "optimizer = torch.optim.Adam(bert_classifier.parameters(), lr=2e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "sequence_length = 32   ## Maximum length of tokens to be used at a time\n",
    "batch_size = 32  ##The number of training examples in one forward/backward pass\n",
    "input_dim = 512  ##The total number of dimension we will allow the model to use for calculation\n",
    "d_model = 512  ##Number of expected features, set to default recommended by pytorch\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attention_mask_test, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_mask_train, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "in_features = input_ids_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available?  True\n",
      "CUDA version:    12.1\n",
      "Device name:     NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"CUDA available? \", torch.cuda.is_available())\n",
    "print(\"CUDA version:   \", torch.version.cuda)\n",
    "print(\"Device name:    \", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bert to classify the data --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.2275, Accuracy: 0.6058\n",
      "Epoch [2/20], Loss: 1.0847, Accuracy: 0.6512\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    bert_classifier,\n",
    "    input_ids_train,\n",
    "    attention_mask_train,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_epochs\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_classifier.state_dict(), 'Bert_Classifier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild full model\n",
    "bert_base = DualBertModel(config)\n",
    "bert_classifier = BertClassifier(bert_model=bert_base, hidden_dim=64, output_dim=num_classes)\n",
    "\n",
    "# Load the model state dict\n",
    "bert_classifier.load_state_dict(torch.load(\"bert_classifier.pt\", map_location=device))\n",
    "\n",
    "# Move to device and set to eval mode\n",
    "bert_classifier.to(device)\n",
    "bert_classifier.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier.eval()\n",
    "test(bert_classifier, test_loader, criterion, device, test_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Test] Epoch [5/5], Loss: 1.3933, Accuracy: 0.7071\n",
    "\n",
    "Epochs\tAccuracy\n",
    "0\t1\t0.707078\n",
    "1\t2\t0.707078\n",
    "2\t3\t0.707078\n",
    "3\t4\t0.707078\n",
    "4\t5\t0.707078\n",
    "\n",
    "\n",
    "Without normalizing inputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Without normalizing inputs, the accuracy can be high but the loss is low.  Run again with the normalized input to see if it helps with loss.\n",
    "[Test] Epoch [5/5], Loss: 1.8423, Accuracy: 0.7026\n",
    "\n",
    "Normalizing the input resulted in both worse Loss and Accuracy.\n",
    "[Test] Epoch [5/5], Loss: 3.0480, Accuracy: 0.5036\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
