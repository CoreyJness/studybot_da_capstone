{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the functions we will be using during our model training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a loop to add the txt files into a data frame\n",
    "def txt_retrieval(folder_path):\n",
    "    qa = []\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "    for file in txt_files:\n",
    "        file_path = os.path.join(folder_path, file) \n",
    "        df = pd.read_json(file_path) \n",
    "        df[\"source_file\"] = file  \n",
    "        qa.append(df)\n",
    "    return pd.concat(qa, ignore_index=True) if qa else pd.DataFrame()\n",
    "\n",
    "\n",
    "#Combine questions and answers to pass to the model\n",
    "def qa_pairs(questions, options):\n",
    "    pairs = []\n",
    "    for q, opts in zip(questions, options):\n",
    "        for opt in opts:\n",
    "            pairs.append((q,opt))\n",
    "    return pairs\n",
    "\n",
    "\n",
    "##Use the tokenizer to encode the text\n",
    "def encode(data_component):\n",
    "        encoded_data = tokenizer(data_component, return_tensors='pt', padding=True)\n",
    "        return encoded_data\n",
    "\n",
    "\n",
    "##Computes scaled dot product attention on query, key and value tensors (pytorch docs)\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim = -1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    preds = torch.argmax(predictions, dim=1)  # Get class with highest probability\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "\n",
    "##Training Loop\n",
    "def train(bert, device, training_data, criterion, optimizer, epoch):\n",
    "    #Set Model to training mode\n",
    "    bert.train()\n",
    "    # Initialize the running loss and accuracy\n",
    "    training_acc_list = []\n",
    "    training_loss = 0.0\n",
    "    training_acc = 0.0\n",
    "    #loop over the batches of data:\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        #move the inputs and labels to the device\n",
    "        inputs = {\n",
    "        \"input_ids\": inputs.to(device),\n",
    "        \"attention_mask\": (inputs != 0).long().to(device)\n",
    "    }\n",
    "        labels = labels.to(device)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        outputs = bert_classifier(inputs) #get the output lgits from the model\n",
    "        loss = criterion(outputs, labels) # Calculate the loss\n",
    "        #Backward pass and optimize\n",
    "        loss.backward() #compute the gradients\n",
    "        optimizer.step() # Update the parameters\n",
    "        #Print the statistics\n",
    "        training_loss += loss.item()\n",
    "        training_acc+= accuracy(outputs, labels)# accumulate the accuracy and loss\n",
    "        if (i + 1) % 200 == 0: # print every two hundred batches\n",
    "            print(f'Epoch {epoch}, Batch {i+1}, Loss: {training_loss / 200:.4f}, Accuracy: {training_acc / 200:.4f}')\n",
    "\n",
    "            training_loss = 0.0\n",
    "            training_acc = 0.0\n",
    "\n",
    "#Testing Loop        \n",
    "def test(bert, device, training_data, criterion, epoch):\n",
    "    # Set the model to evaluation mode\n",
    "    bert.eval()\n",
    "    #Intitialize loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    # Loop over the batches of data\n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in training_data:\n",
    "            #move the inputs and labels to the \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = bert(inputs) # Get the output logits from the model\n",
    "            loss = criterion(outputs, labels) # Calculate the loss\n",
    "            # Print the statistics\n",
    "            test_loss += loss.item() # Accumulate the loss\n",
    "            test_acc += accuracy(outputs, labels) # Accumulate the accuracy\n",
    "    \n",
    "    accuracy_x_epoch = pd.test_acc({test_acc}, index=epoch)\n",
    "    \n",
    "    \n",
    "    # Print the average loss and accuracy\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_acc / len(test_loader):.4f}')\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the question sets that will be used to train the model.  The first dataset is the RACE dataseet, which consists of multiple choice questions separated between M (middle school) and H (high school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "middle = \"middle\"\n",
    "high = \"high\\high\"\n",
    "\n",
    "\n",
    "# Assign separate outputs based on the variable names\n",
    "m_qa = txt_retrieval(middle)\n",
    "h_qa = txt_retrieval(high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the column names to see how the data is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_qa.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_questions = m_qa.questions.values.tolist()\n",
    "h_questions = h_qa.questions.values.tolist()\n",
    "m_options = m_qa.options.values.tolist()\n",
    "h_options = h_qa.options.values.tolist()\n",
    "m_article = m_qa.article.values.tolist()\n",
    "h_article = h_qa.article.values.tolist()\n",
    "m_id = m_qa.id.values.tolist()\n",
    "h_id = h_qa.id.values.tolist()\n",
    "m_answers = m_qa.answers.values.tolist()\n",
    "h_answers = h_qa.answers.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Bert from transformers and use the tokenizer specialized for the model to input text as tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertConfig, BertModel, AutoModel, AutoTokenizer\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  \n",
    "\n",
    "\n",
    "#m_qa_pairs = qa_pairs(m_questions, m_options)\n",
    "#m_qa_pairs\n",
    "\n",
    "\n",
    "\n",
    "#m_qa_inputs = encode(m_qa_pairs)\n",
    "#torch.save(m_qa_inputs, \"m_qa_inputs.pt\")\n",
    "\n",
    "\n",
    "#m_answers_inputs = encode(m_answers)\n",
    "#torch.save(m_answers_inputs, \"m_answers_inputs.pt\")\n",
    "\n",
    "\n",
    "#m_readings_inputs = encode(m_article)\n",
    "#torch.save(m_readings_inputs, \"m_readings_inputs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode questions, option, and answer components.   Save the encoded QA inputs to avoid the time consumption required from the tokenization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h_qa_pairs = qa_pairs(h_questions, h_options)\n",
    "#h_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#h_qa_inputs = encode(h_qa_pairs)\n",
    "#torch.save(h_qa_inputs, \"h_qa_inputs.pt\")\n",
    "\n",
    "\n",
    "#h_answers_inputs = encode(h_answers)\n",
    "#torch.save(h_answers_inputs, \"h_answers_inputs.pt\")\n",
    "\n",
    "\n",
    "#h_readings_inputs = encode(h_article)\n",
    "#torch.save(h_readings_inputs, \"h_readings_inputs.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start training our model by using two Multi-Head Attention networks to compare the questions and the answer sequences.  We will first set up the training parameters for the networks. \n",
    "\n",
    "Sequence length is set to be the max size input in the high school dataset.  \n",
    "Batch Size is the number of times that the network will run through the data in a training session.\n",
    "Input dim is the vector dimension.  This sets the number of dimensions that the network uses.  \n",
    "D model is the output of attention model for all of the inputs\n",
    "m qa training is coming from the tokenized questions and answers that we saved earlier to the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_qa['article'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I load in and label the data from before and split the data into training, testing, and validation data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split \n",
    "m_qa_inputs = torch.load(\"m_qa_inputs.pt\", weights_only=False)\n",
    "h_qa_inputs = torch.load(\"h_qa_inputs.pt\", weights_only=False)\n",
    "m_qa_tensors = m_qa_inputs['input_ids']\n",
    "h_qa_tensors = h_qa_inputs['input_ids']\n",
    "m_labels = torch.zeros(len(m_qa_tensors), dtype=torch.long)  # Middle school = 0\n",
    "h_labels = torch.ones(len(h_qa_tensors), dtype=torch.long)   #High school = 1\n",
    "labels = torch.cat([m_labels, h_labels], dim=0)\n",
    "\n",
    "max_seq_len = max(m_qa_tensors.shape[1], h_qa_tensors.shape[1])\n",
    "m_qa_tensors = torch.nn.functional.pad(m_qa_tensors, (0, max_seq_len - m_qa_tensors.shape[1]))\n",
    "h_qa_tensors = torch.nn.functional.pad(h_qa_tensors, (0, max_seq_len - h_qa_tensors.shape[1]))\n",
    "training_data = torch.cat([m_qa_tensors, h_qa_tensors], dim=0)\n",
    "total_size=training_data.shape[0]\n",
    "train_size = int(.6 * total_size)\n",
    "val_size = int(.3 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_data, val_data, test_data = random_split(training_data, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention_1(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * self.head_dim)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        \n",
    "        qkv = self.qkv_layer(x)\n",
    "        \n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        \n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        \n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        \n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        \n",
    "        h_1 = self.linear_layer(values)\n",
    "       \n",
    "        return h_1\n",
    "\n",
    "class MultiheadAttention_2(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim , 3 * self.head_dim)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, input_dim = x.size()\n",
    "        \n",
    "        qkv = self.qkv_layer(x)\n",
    "        \n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "\n",
    "        h = self.linear_layer(values)\n",
    "\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your hyperparameters for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "epochs = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer  = optim.Adam(bert.parameters(), lr=2e-5, betas=(0.9, 0.999), weight_decay=1e-5)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "sequence_length = 3714\n",
    "batch_size = 50\n",
    "input_dim = 500\n",
    "d_model = 512\n",
    "m_qa_training_parameters = torch.randn(input_dim, batch_size, sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the data in a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "dataset = TensorDataset(training_data, labels) \n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape, training_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function for training and pass the parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert, hidden_dim=768, num_classes=2):  # Adjust num_classes as needed\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)  # Maps hidden_dim to num_classes\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits = self.classifier(pooled_output)  # Pass pooled output to classifier\n",
    "        return logits  # Now shaped (batch_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_classifier = BertClassifier(bert).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(bert_classifier, device, trainloader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model weights to use the classifier at another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(bert_classifier.state_dict(), 'Bert_Classifier')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
